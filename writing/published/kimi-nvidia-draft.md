# 《性能翻倍！手把手教你在 Claude Code 里免费跑通 Kimi K2.5 和 GLM-4.7》

最近大模型圈最硬核的消息，莫过于月之暗面的 **Kimi K2.5** 悄悄上线了 NVIDIA build.nvidia.com 平台。

这意味这什么？意味着这个号称能跟 Claude 3.5 Sonnet 掰手腕的顶级国产模型，现在可以**免费调用 API 了**！

但很多小伙伴反应：直接调 API 只能写简单的脚本，想在 **Claude Code** 这种顶尖的 AI 编程工具里用，协议对不上，速度还不稳。

别急，**NovaProxy** 迎来了重磅更新！今天我就教大家如何用这个“神器”，一键打通 NVIDIA 顶级算力，让你的 Claude Code 彻底飞起来。

---

## 为什么要用 NovaProxy？

如果你直接去调 NVIDIA 的接口，可能会遇到几个头疼的问题：
1. **协议不兼容**：Claude Code 只认 Anthropic 协议。
2. **连接不稳定**：跨海请求容易断连。
3. **额度限制**：单个 Key 很快就触发频率限制。

**NovaProxy 的这次“生产力级”升级，一次性把这些全解决了：**

- **⚡️ 极速流式输出**：全新的 `StreamTranslator` 架构，把 OpenAI 信号由于“零延迟”翻译给 Claude，打字机效果丝滑到飞起。
- **🔄 多 Key 轮询**：你现在可以拍进去十个 Key，系统自动负载均衡，彻底告别 429 报错。
- **🛡️ 工业级稳定性**：内置了自动连接池复用和指数退避重试机制。网络抖动？不存在的，它会自动重连直到成功。
- **📊 颜值即正义**：那个暖沙色的 UI 仪表盘不仅好看，还能实时监控你白嫖了多少 Token。

---

## 保姆级配置教程

### 第一步：获取“免费算力”

去 NVIDIA NIM 平台（build.nvidia.com），直接搜 **Kimi K2.5** 或者 **GLM-4.7**。
注册不需要外币卡，找到 API Key，那个 `nvapi-xxx` 格式的字符串就是我们要的“能量块”。（建议多申请几个，NovaProxy 支持轮询！）

### 第二步：架起“跨海大桥”

1. 下载并运行最新的 **NovaProxy.exe**。
2. 浏览器会自动弹出那个精致的“暖沙色”后台。
3. 在设置里把你的多个 NVIDIA Key 全部填进去，用逗号隔开。
4. **重点**：在模型映射里，把 `claude-3-5-sonnet` 映射给 `moonshotai/kimi-aigo-k2.5` 或者 `智谱/glm-4-9b-instruct`（根据你喜欢的选）。

### 第三步：暴力破拆——cc-switch 登场

为了让 Claude Code 乖乖听话，我们需要用 `cc-switch` 切换一下地址：

```bash
# 1. 添加 Nova 节点
cc-switch add Nova-Kimi
# 2. 填写地址：http://localhost:3001/v1
# 3. 一键切换
cc-switch switch Nova-Kimi
```

现在，回到你的代码目录，输入 `claude`。恭喜你，你正在用着 NVIDIA 的免费 GPU 算力，跑着国产最强的 Kimi K2.5，驱动着全球最顶尖的编程 CLI。

---

## 性能实测

我用这套方案跑了一下午的代码改造，体验如下：

- **响应速度**：得益于 `httpx` 连接池的优化，首字响应（TTFT）感官上提升了 40% 左右。
- **稳定性**：之前偶尔遇到的 `ReadTimeout` 消失了，重试机制在后台默默守护，体验极其丝滑。
- **逻辑能力**：Kimi K2.5 的中文理解确实顶，改起业务逻辑来比原生英文模型更懂我的意图。

---

## 写在最后

大模型时代，工具的边界正在消失。

通过 **NovaProxy** 解决协议适配，利用 **NVIDIA** 的生态红利，我们不仅省下了昂贵的 API 费用，更实现了一种“算力自由”。

国产大模型如 Kimi、GLM 已经展现了不俗的力量，而我们需要做的，就是用最好的工具把这种力量释放出来。

**快去试试吧，别让那些免费的算力在后台躺着吃灰！**

---

*如果你在配置过程中遇到任何问题，欢迎在评论区留言。如果你觉得这篇教程有用，点个“在看”就是对我最大的支持！*

